{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.23 64-bit ('quantech': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0edbfc4fd16687e9fa556001b096f3c5bef9555faada60aa4e49d2442123d6b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "import optuna\n",
    "import argparse\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "\n",
    "from utils import * \n",
    "from models import MODEL_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial experimental settings\n",
    "results_dir = 'GIN-sota'\n",
    "os.makedirs(results_dir, exist_ok=True) #Output directory to save results and model weights\n",
    "batch_size = 512\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, total_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to('cpu')\n",
    "            out = model(batch.x, batch.edge_index)[batch.batch_size:]\n",
    "            loss = loss_fn(out, batch.y[batch.batch_size:])\n",
    "            total_loss += loss.item() * batch.batch_size\n",
    "            all_preds.append(out.argmax(dim=1))\n",
    "            all_labels.append(batch.y[batch.batch_size:])\n",
    "    y_true = torch.cat(all_labels)\n",
    "    y_pred = torch.cat(all_preds)\n",
    "    acc = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "    return total_loss / len(loader.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'hidden_channels': 512, 'num_neighbors': 5, 'dropout': 0.2690851529012735, 'lr': 0.002025225495945881, 'num_layers': 3, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'gin'\n",
    "model_config_path = 'best_model_config.json'\n",
    "# Load the JSON content\n",
    "with open(model_config_path, 'r') as f:\n",
    "    best_params = json.load(f)['params']\n",
    "# Optional: print or inspect the keys\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['num_neighbors'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_adata = sc.read_h5ad('../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151507.h5ad')\n",
    "X = train_adata.X.toarray() if not isinstance(train_adata.X, np.ndarray) else adata.X\n",
    "Y = LabelEncoder().fit_transform(train_adata.obs['Region'])\n",
    "\n",
    "construct_interaction_KNN(train_adata, n_neighbors=best_params['num_neighbors'])  \n",
    "adj_spatial = train_adata.obsm['adj']\n",
    "edge_index = dense_to_sparse_edge_index(adj_spatial)\n",
    "\n",
    "train_idx, eval_idx = train_test_split(np.arange(len(Y)), test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "data = Data(x=torch.tensor(X, dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor(Y, dtype=torch.long))\n",
    "data.train_mask = torch.zeros(len(Y), dtype=torch.bool)\n",
    "data.train_mask[train_idx] = True\n",
    "data.eval_mask = torch.zeros(len(Y), dtype=torch.bool)\n",
    "data.eval_mask[eval_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GIN(\n  (convs): ModuleList(\n    (0): GINConv(nn=Sequential(\n      (0): Linear(in_features=3266, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=512, bias=True)\n    ))\n    (1): GINConv(nn=Sequential(\n      (0): Linear(in_features=512, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=512, bias=True)\n    ))\n    (2): GINConv(nn=Sequential(\n      (0): Linear(in_features=512, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=7, bias=True)\n    ))\n  )\n)\n"
     ]
    }
   ],
   "source": [
    " # ========== 5. Final Training ==========\n",
    "\n",
    "config_str = f\"{selected_model}_h{best_params['hidden_channels']}_l{best_params['num_layers']}_d{best_params['dropout']:.2f}_lr{best_params['lr']:.0e}\"\n",
    "best_model_path = os.path.join(results_dir, f\"{config_str}.pth\")\n",
    "\n",
    "model = MODEL_CLASSES[selected_model](\n",
    "    in_channels=X.shape[1],\n",
    "    hidden_channels=best_params['hidden_channels'],\n",
    "    out_channels=np.unique(Y).shape[0],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "al Acc: 0.9615\n",
      "Epoch 93 | Train Acc: 0.9950 | Eval Acc: 0.9942\n",
      "Epoch 94 | Train Acc: 0.9837 | Eval Acc: 0.9832\n",
      "Epoch 95 | Train Acc: 0.9986 | Eval Acc: 0.9982\n",
      "Epoch 96 | Train Acc: 0.9999 | Eval Acc: 0.9997\n",
      "Epoch 97 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 98 | Train Acc: 0.9999 | Eval Acc: 0.9997\n",
      "Epoch 99 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 100 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 101 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 102 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 103 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 104 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 105 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 106 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 107 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 108 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 109 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 110 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 111 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 112 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 113 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 114 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 115 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 116 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 117 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 118 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 119 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 120 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 121 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 122 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 123 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 124 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 125 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 126 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 127 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 128 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 129 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 130 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 131 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 132 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 133 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 134 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 135 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 136 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 137 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 138 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 139 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 140 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 141 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 142 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 143 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 144 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 145 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 146 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 147 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 148 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 149 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 150 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 151 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 152 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 153 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 154 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 155 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 156 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 157 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 158 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 159 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 160 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 161 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 162 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 163 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 164 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 165 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 166 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 167 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 168 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 169 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 170 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 171 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 172 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 173 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 174 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 175 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 176 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 177 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 178 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 179 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 180 | Train Acc: 0.9998 | Eval Acc: 0.9998\n",
      "Epoch 181 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 182 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 183 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 184 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 185 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 186 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 187 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 188 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 189 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 190 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 191 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 192 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 193 | Train Acc: 0.9998 | Eval Acc: 0.9998\n",
      "Epoch 194 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 195 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 196 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 197 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 198 | Train Acc: 0.9997 | Eval Acc: 0.9998\n",
      "Epoch 199 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 200 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 201 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 202 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 203 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 204 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 205 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 206 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 207 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 208 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 209 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 210 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 211 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 212 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 213 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 214 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 215 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 216 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 217 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 218 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 219 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 220 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 221 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 222 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 223 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 224 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 225 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 226 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 227 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 228 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 229 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 230 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 231 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 232 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 233 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 234 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 235 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 236 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 237 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 238 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 239 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 240 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 241 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 242 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 243 | Train Acc: 0.9994 | Eval Acc: 0.9997\n",
      "Epoch 244 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 245 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 246 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 247 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 248 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 249 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 250 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 251 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 252 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 253 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 254 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 255 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 256 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 257 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 258 | Train Acc: 0.9985 | Eval Acc: 0.9987\n",
      "Epoch 259 | Train Acc: 0.4800 | Eval Acc: 0.4731\n",
      "Epoch 260 | Train Acc: 0.4756 | Eval Acc: 0.4666\n",
      "Epoch 261 | Train Acc: 0.6391 | Eval Acc: 0.6313\n",
      "Epoch 262 | Train Acc: 0.7664 | Eval Acc: 0.7602\n",
      "Epoch 263 | Train Acc: 0.7828 | Eval Acc: 0.7791\n",
      "Epoch 264 | Train Acc: 0.8983 | Eval Acc: 0.8911\n",
      "Epoch 265 | Train Acc: 0.9268 | Eval Acc: 0.9208\n",
      "Epoch 266 | Train Acc: 0.9030 | Eval Acc: 0.8985\n",
      "Epoch 267 | Train Acc: 0.7758 | Eval Acc: 0.7716\n",
      "Epoch 268 | Train Acc: 0.8720 | Eval Acc: 0.8685\n",
      "Epoch 269 | Train Acc: 0.9610 | Eval Acc: 0.9586\n",
      "Epoch 270 | Train Acc: 0.9744 | Eval Acc: 0.9727\n",
      "Epoch 271 | Train Acc: 0.9788 | Eval Acc: 0.9768\n",
      "Epoch 272 | Train Acc: 0.9886 | Eval Acc: 0.9870\n",
      "Epoch 273 | Train Acc: 0.9920 | Eval Acc: 0.9903\n",
      "Epoch 274 | Train Acc: 0.9967 | Eval Acc: 0.9958\n",
      "Epoch 275 | Train Acc: 0.9997 | Eval Acc: 0.9997\n",
      "Epoch 276 | Train Acc: 0.9994 | Eval Acc: 0.9990\n",
      "Epoch 277 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 278 | Train Acc: 0.9996 | Eval Acc: 0.9997\n",
      "Epoch 279 | Train Acc: 0.9995 | Eval Acc: 0.9995\n",
      "Epoch 280 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 281 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 282 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 283 | Train Acc: 0.9998 | Eval Acc: 0.9998\n",
      "Epoch 284 | Train Acc: 0.9998 | Eval Acc: 0.9998\n",
      "Epoch 285 | Train Acc: 0.9997 | Eval Acc: 0.9998\n",
      "Epoch 286 | Train Acc: 0.9997 | Eval Acc: 0.9998\n",
      "Epoch 287 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 288 | Train Acc: 0.9996 | Eval Acc: 0.9997\n",
      "Epoch 289 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 290 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 291 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 292 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 293 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 294 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 295 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 296 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 297 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 298 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 299 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 300 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 301 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 302 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 303 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 304 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 305 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 306 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 307 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 308 | Train Acc: 0.9999 | Eval Acc: 0.9997\n",
      "Epoch 309 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 310 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 311 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 312 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 313 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 314 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 315 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 316 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 317 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 318 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 319 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 320 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 321 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 322 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 323 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 324 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 325 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 326 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 327 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 328 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 329 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 330 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 331 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 332 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 333 | Train Acc: 0.9993 | Eval Acc: 0.9993\n",
      "Epoch 334 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 335 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 336 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 337 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 338 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 339 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 340 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 341 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 342 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 343 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 344 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 345 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 346 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 347 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 348 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 349 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 350 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 351 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 352 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 353 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 354 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 355 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 356 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 357 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 358 | Train Acc: 0.9997 | Eval Acc: 0.9997\n",
      "Epoch 359 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 360 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 361 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 362 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 363 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 364 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 365 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 366 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 367 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 368 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 369 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 370 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 371 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 372 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 373 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 374 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 375 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 376 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 377 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 378 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 379 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 380 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 381 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 382 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 383 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 384 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 385 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 386 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 387 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 388 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 389 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 390 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 391 | Train Acc: 0.9997 | Eval Acc: 0.9998\n",
      "Epoch 392 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 393 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 394 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 395 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 396 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 397 | Train Acc: 1.0000 | Eval Acc: 1.0000\n",
      "Epoch 398 | Train Acc: 0.9998 | Eval Acc: 0.9997\n",
      "Epoch 399 | Train Acc: 0.9800 | Eval Acc: 0.9818\n",
      "Epoch 400 | Train Acc: 0.8202 | Eval Acc: 0.8115\n",
      "Epoch 401 | Train Acc: 0.9303 | Eval Acc: 0.9302\n",
      "Epoch 402 | Train Acc: 0.9649 | Eval Acc: 0.9651\n",
      "Epoch 403 | Train Acc: 0.9329 | Eval Acc: 0.9322\n",
      "Epoch 404 | Train Acc: 0.9747 | Eval Acc: 0.9725\n",
      "Epoch 405 | Train Acc: 0.9584 | Eval Acc: 0.9591\n",
      "Epoch 406 | Train Acc: 0.9779 | Eval Acc: 0.9766\n",
      "Epoch 407 | Train Acc: 0.9814 | Eval Acc: 0.9806\n",
      "Epoch 408 | Train Acc: 0.9573 | Eval Acc: 0.9558\n",
      "Epoch 409 | Train Acc: 0.9590 | Eval Acc: 0.9586\n",
      "Epoch 410 | Train Acc: 0.9829 | Eval Acc: 0.9830\n",
      "Epoch 411 | Train Acc: 0.9748 | Eval Acc: 0.9755\n",
      "Epoch 412 | Train Acc: 0.9712 | Eval Acc: 0.9718\n",
      "Epoch 413 | Train Acc: 0.9795 | Eval Acc: 0.9786\n",
      "Epoch 414 | Train Acc: 0.9650 | Eval Acc: 0.9660\n",
      "Epoch 415 | Train Acc: 0.9686 | Eval Acc: 0.9686\n",
      "Epoch 416 | Train Acc: 0.9712 | Eval Acc: 0.9723\n",
      "Epoch 417 | Train Acc: 0.9771 | Eval Acc: 0.9768\n",
      "Epoch 418 | Train Acc: 0.9689 | Eval Acc: 0.9686\n",
      "Epoch 419 | Train Acc: 0.9648 | Eval Acc: 0.9645\n",
      "Epoch 420 | Train Acc: 0.9670 | Eval Acc: 0.9674\n",
      "Epoch 421 | Train Acc: 0.9927 | Eval Acc: 0.9930\n",
      "Epoch 422 | Train Acc: 0.9859 | Eval Acc: 0.9865\n",
      "Epoch 423 | Train Acc: 0.9657 | Eval Acc: 0.9662\n",
      "Epoch 424 | Train Acc: 0.9763 | Eval Acc: 0.9760\n",
      "Epoch 425 | Train Acc: 0.9679 | Eval Acc: 0.9670\n",
      "Epoch 426 | Train Acc: 0.9703 | Eval Acc: 0.9690\n",
      "Epoch 427 | Train Acc: 0.9881 | Eval Acc: 0.9868\n",
      "Epoch 428 | Train Acc: 0.9773 | Eval Acc: 0.9780\n",
      "Epoch 429 | Train Acc: 0.9827 | Eval Acc: 0.9815\n",
      "Epoch 430 | Train Acc: 0.9193 | Eval Acc: 0.9168\n",
      "Epoch 431 | Train Acc: 0.8437 | Eval Acc: 0.8407\n",
      "Epoch 432 | Train Acc: 0.9823 | Eval Acc: 0.9818\n",
      "Epoch 433 | Train Acc: 0.9567 | Eval Acc: 0.9568\n",
      "Epoch 434 | Train Acc: 0.9647 | Eval Acc: 0.9621\n",
      "Epoch 435 | Train Acc: 0.9663 | Eval Acc: 0.9671\n",
      "Epoch 436 | Train Acc: 0.9705 | Eval Acc: 0.9715\n",
      "Epoch 437 | Train Acc: 0.9838 | Eval Acc: 0.9850\n",
      "Epoch 438 | Train Acc: 0.9671 | Eval Acc: 0.9683\n",
      "Epoch 439 | Train Acc: 0.9532 | Eval Acc: 0.9555\n",
      "Epoch 440 | Train Acc: 0.9834 | Eval Acc: 0.9847\n",
      "Epoch 441 | Train Acc: 0.9530 | Eval Acc: 0.9563\n",
      "Epoch 442 | Train Acc: 0.9612 | Eval Acc: 0.9645\n",
      "Epoch 443 | Train Acc: 0.9579 | Eval Acc: 0.9615\n",
      "Epoch 444 | Train Acc: 0.9486 | Eval Acc: 0.9507\n",
      "Epoch 445 | Train Acc: 0.9430 | Eval Acc: 0.9463\n",
      "Epoch 446 | Train Acc: 0.9610 | Eval Acc: 0.9624\n",
      "Epoch 447 | Train Acc: 0.9116 | Eval Acc: 0.9131\n",
      "Epoch 448 | Train Acc: 0.6888 | Eval Acc: 0.6869\n",
      "Epoch 449 | Train Acc: 0.8942 | Eval Acc: 0.8912\n",
      "Epoch 450 | Train Acc: 0.9787 | Eval Acc: 0.9783\n",
      "Epoch 451 | Train Acc: 0.9827 | Eval Acc: 0.9823\n",
      "Epoch 452 | Train Acc: 0.9781 | Eval Acc: 0.9775\n",
      "Epoch 453 | Train Acc: 0.9832 | Eval Acc: 0.9823\n",
      "Epoch 454 | Train Acc: 0.9841 | Eval Acc: 0.9827\n",
      "Epoch 455 | Train Acc: 0.9769 | Eval Acc: 0.9760\n",
      "Epoch 456 | Train Acc: 0.9771 | Eval Acc: 0.9777\n",
      "Epoch 457 | Train Acc: 0.9678 | Eval Acc: 0.9690\n",
      "Epoch 458 | Train Acc: 0.9738 | Eval Acc: 0.9739\n",
      "Epoch 459 | Train Acc: 0.9855 | Eval Acc: 0.9855\n",
      "Epoch 460 | Train Acc: 0.9653 | Eval Acc: 0.9670\n",
      "Epoch 461 | Train Acc: 0.9915 | Eval Acc: 0.9908\n",
      "Epoch 462 | Train Acc: 0.9730 | Eval Acc: 0.9728\n",
      "Epoch 463 | Train Acc: 0.9790 | Eval Acc: 0.9780\n",
      "Epoch 464 | Train Acc: 0.9912 | Eval Acc: 0.9907\n",
      "Epoch 465 | Train Acc: 0.9944 | Eval Acc: 0.9943\n",
      "Epoch 466 | Train Acc: 0.9623 | Eval Acc: 0.9616\n",
      "Epoch 467 | Train Acc: 0.9939 | Eval Acc: 0.9933\n",
      "Epoch 468 | Train Acc: 0.9901 | Eval Acc: 0.9898\n",
      "Epoch 469 | Train Acc: 0.9816 | Eval Acc: 0.9805\n",
      "Epoch 470 | Train Acc: 0.9973 | Eval Acc: 0.9972\n",
      "Epoch 471 | Train Acc: 0.9939 | Eval Acc: 0.9932\n",
      "Epoch 472 | Train Acc: 0.9940 | Eval Acc: 0.9937\n",
      "Epoch 473 | Train Acc: 0.9775 | Eval Acc: 0.9773\n",
      "Epoch 474 | Train Acc: 0.9880 | Eval Acc: 0.9865\n",
      "Epoch 475 | Train Acc: 0.9946 | Eval Acc: 0.9942\n",
      "Epoch 476 | Train Acc: 0.9877 | Eval Acc: 0.9865\n",
      "Epoch 477 | Train Acc: 0.9923 | Eval Acc: 0.9913\n",
      "Epoch 478 | Train Acc: 0.9806 | Eval Acc: 0.9794\n",
      "Epoch 479 | Train Acc: 0.9839 | Eval Acc: 0.9818\n",
      "Epoch 480 | Train Acc: 0.9914 | Eval Acc: 0.9900\n",
      "Epoch 481 | Train Acc: 0.9884 | Eval Acc: 0.9862\n",
      "Epoch 482 | Train Acc: 0.9878 | Eval Acc: 0.9863\n",
      "Epoch 483 | Train Acc: 0.9900 | Eval Acc: 0.9894\n",
      "Epoch 484 | Train Acc: 0.9836 | Eval Acc: 0.9825\n",
      "Epoch 485 | Train Acc: 0.9739 | Eval Acc: 0.9719\n",
      "Epoch 486 | Train Acc: 0.9916 | Eval Acc: 0.9902\n",
      "Epoch 487 | Train Acc: 0.9778 | Eval Acc: 0.9769\n",
      "Epoch 488 | Train Acc: 0.9966 | Eval Acc: 0.9962\n",
      "Epoch 489 | Train Acc: 0.9980 | Eval Acc: 0.9977\n",
      "Epoch 490 | Train Acc: 0.9934 | Eval Acc: 0.9931\n",
      "Epoch 491 | Train Acc: 0.9972 | Eval Acc: 0.9965\n",
      "Epoch 492 | Train Acc: 0.9963 | Eval Acc: 0.9957\n",
      "Epoch 493 | Train Acc: 0.9834 | Eval Acc: 0.9828\n",
      "Epoch 494 | Train Acc: 0.9781 | Eval Acc: 0.9781\n",
      "Epoch 495 | Train Acc: 0.9882 | Eval Acc: 0.9868\n",
      "Epoch 496 | Train Acc: 0.9806 | Eval Acc: 0.9800\n",
      "Epoch 497 | Train Acc: 0.9788 | Eval Acc: 0.9793\n",
      "Epoch 498 | Train Acc: 0.9859 | Eval Acc: 0.9852\n",
      "Epoch 499 | Train Acc: 0.9827 | Eval Acc: 0.9833\n",
      "Epoch 500 | Train Acc: 0.9956 | Eval Acc: 0.9945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer_name = best_params.get('optimizer', 'AdamW')  # Default to AdamW if not present\n",
    "\n",
    "if optimizer_name == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "elif optimizer_name == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'])\n",
    "elif optimizer_name == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=best_params['lr'], momentum=0.9)\n",
    "elif optimizer_name == 'RMSprop':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "                            num_neighbors=[best_params['num_neighbors']], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "eval_loader = NeighborLoader(data, input_nodes=data.eval_mask,\n",
    "                            num_neighbors=[best_params['num_neighbors']], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "train_log = []\n",
    "best_eval_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)[batch.batch_size:]\n",
    "        loss = loss_fn(out, batch.y[batch.batch_size:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = evaluate(model, train_loader, loss_fn)\n",
    "    eval_loss, eval_acc = evaluate(model, eval_loader, loss_fn)\n",
    "\n",
    "    if eval_acc > best_eval_acc:\n",
    "        best_eval_acc = eval_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    train_log.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'eval_loss': eval_loss,\n",
    "        'eval_acc': eval_acc\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Eval Acc: {eval_acc:.4f}\")\n",
    "# Save logs\n",
    "with open(os.path.join(results_dir, 'train_log.json'), 'w') as f:\n",
    "    json.dump(train_log, f, indent=2)\n",
    "\n",
    "with open(os.path.join(results_dir, 'best_model_config.json'), 'w') as f:\n",
    "    json.dump({'best_model_path': best_model_path, 'params': best_params}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Loaded best model from GIN-sota/gin_h512_l3_d0.27_lr2e-03.pth\n",
      "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating:  33%|███▎      | 1/3 [00:02<00:04,  2.14s/it]Test PID 151508 | Accuracy: 0.5755\n",
      "Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating:  67%|██████▋   | 2/3 [00:04<00:02,  2.31s/it]Test PID 151509 | Accuracy: 0.6134\n",
      "Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating: 100%|██████████| 3/3 [00:06<00:00,  2.27s/it]Test PID 151510 | Accuracy: 0.5905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== 6. Test ==========\n",
    "with open(os.path.join(results_dir, 'best_model_config.json')) as f:\n",
    "    best_model_meta = json.load(f)\n",
    "\n",
    "model = MODEL_CLASSES[selected_model](\n",
    "    in_channels=X.shape[1],\n",
    "    hidden_channels=best_model_meta['params']['hidden_channels'],\n",
    "    out_channels=np.unique(Y).shape[0],\n",
    "    num_layers=best_model_meta['params']['num_layers'],\n",
    "    dropout=best_model_meta['params']['dropout']\n",
    ")\n",
    "model.load_state_dict(torch.load(best_model_meta['best_model_path']))\n",
    "print(f\"\\nLoaded best model from {best_model_meta['best_model_path']}\")\n",
    "\n",
    "test_adata_paths = [\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151508.h5ad',\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151509.h5ad',\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151510.h5ad',\n",
    "]\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for path in tqdm(test_adata_paths, desc=\"Evaluating\"):\n",
    "    test_pid = path.split('/')[-1].split('.')[0]\n",
    "    adata_test = sc.read_h5ad(path)\n",
    "    X_test = adata_test.X.toarray() if not isinstance(adata_test.X, np.ndarray) else adata_test.X\n",
    "    Y_test = LabelEncoder().fit_transform(adata_test.obs['Region'])\n",
    "\n",
    "    construct_interaction_KNN(adata_test, n_neighbors=best_params['num_neighbors'])  \n",
    "    adj_spatial_test = adata_test.obsm['adj']\n",
    "    edge_index_test = dense_to_sparse_edge_index(adj_spatial_test)\n",
    "\n",
    "    data_test = Data(x=torch.tensor(X_test, dtype=torch.float),\n",
    "                    edge_index=edge_index_test,\n",
    "                    y=torch.tensor(Y_test, dtype=torch.long))\n",
    "    data_test.test_mask = torch.ones(data_test.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    test_loader = NeighborLoader(\n",
    "        data_test, input_nodes=data_test.test_mask,\n",
    "        num_neighbors=[best_params['num_neighbors']], batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
    "    test_results[test_pid] = {'loss': float(test_loss), 'accuracy': float(test_acc)}\n",
    "    print(f\"Test PID {test_pid} | Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "with open(os.path.join(results_dir, 'test_results.json'), 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import torch\n",
    "\n",
    "def extract_embeddings_with_hook(model, data):\n",
    "    embeddings = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        embeddings.append(output.detach().cpu())\n",
    "\n",
    "    # Register the hook on model.convs[2].nn[0] (i.e., Linear(512 → 512))\n",
    "    handle = model.convs[2].nn[0].register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass (you don’t care about output here)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        _ = model(data.x, data.edge_index)\n",
    "\n",
    "    # Remove the hook\n",
    "    handle.remove()\n",
    "\n",
    "    # Return the embeddings\n",
    "    return embeddings[0]  # shape: [num_nodes, 512]\n",
    "\n",
    "def test_cluster(model, data, num_classes):\n",
    "    # Step 1: Extract intermediate node embeddings\n",
    "    node_embeddings = extract_embeddings_with_hook(model, data)\n",
    "\n",
    "    # Step 2: Clustering\n",
    "    kmeans = KMeans(n_clusters=num_classes, n_init=10, random_state=42)\n",
    "    pred = kmeans.fit_predict(node_embeddings.numpy())\n",
    "\n",
    "    # Step 3: ARI & NMI\n",
    "    true = data.y.cpu().numpy()\n",
    "    ari = adjusted_rand_score(true, pred)\n",
    "    nmi = normalized_mutual_info_score(true, pred)\n",
    "\n",
    "    print(f\"ARI: {ari:.4f}, NMI: {nmi:.4f}\")\n",
    "    return ari, nmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it]Test PID 151508 | Acc: 0.5750 | ARI: 0.5938 | NMI: 0.7071\n",
      "Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating:  67%|██████▋   | 2/3 [00:08<00:04,  4.46s/it]Test PID 151509 | Acc: 0.6130 | ARI: 0.5914 | NMI: 0.6851\n",
      "Graph constructed with 15 neighbors and stored in adata.obsm['adj'].\n",
      "Evaluating: 100%|██████████| 3/3 [00:13<00:00,  4.36s/it]Test PID 151510 | Acc: 0.5902 | ARI: 0.5306 | NMI: 0.6669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_embeddings_with_hook(model, data, device):\n",
    "    embeddings = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        embeddings.append(output.detach().cpu())\n",
    "\n",
    "    handle = model.convs[2].nn[0].register_forward_hook(hook)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(data.x.to(device), data.edge_index.to(device))\n",
    "\n",
    "    handle.remove()\n",
    "    return embeddings[0]  # shape [N, 512]\n",
    "\n",
    "def run_clustering_eval(embeddings, labels, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    pred = kmeans.fit_predict(embeddings.numpy())\n",
    "\n",
    "    ari = adjusted_rand_score(labels, pred)\n",
    "    nmi = normalized_mutual_info_score(labels, pred)\n",
    "\n",
    "    return float(ari), float(nmi)\n",
    "\n",
    "\n",
    "# === Main evaluation loop with clustering ===\n",
    "test_adata_paths = [\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151508.h5ad',\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151509.h5ad',\n",
    "    '../../../dataset_10x-visium-processed/dataset_10x-visium_filtered_adatas_hvg-16384/151510.h5ad',\n",
    "]\n",
    "\n",
    "test_results = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for path in tqdm(test_adata_paths, desc=\"Evaluating\"):\n",
    "    test_pid = path.split('/')[-1].split('.')[0]\n",
    "    adata_test = sc.read_h5ad(path)\n",
    "    X_test = adata_test.X.toarray() if not isinstance(adata_test.X, np.ndarray) else adata_test.X\n",
    "    Y_test = LabelEncoder().fit_transform(adata_test.obs['Region'])\n",
    "\n",
    "    construct_interaction_KNN(adata_test, n_neighbors=best_params['num_neighbors'])  \n",
    "    adj_spatial_test = adata_test.obsm['adj']\n",
    "    edge_index_test = dense_to_sparse_edge_index(adj_spatial_test)\n",
    "\n",
    "    data_test = Data(\n",
    "        x=torch.tensor(X_test, dtype=torch.float),\n",
    "        edge_index=edge_index_test,\n",
    "        y=torch.tensor(Y_test, dtype=torch.long)\n",
    "    )\n",
    "    data_test.test_mask = torch.ones(data_test.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Run classification evaluation\n",
    "    test_loader = NeighborLoader(\n",
    "        data_test, input_nodes=data_test.test_mask,\n",
    "        num_neighbors=[best_params['num_neighbors']], batch_size=512, shuffle=False, num_workers=0\n",
    "    )\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
    "\n",
    "    # Run embedding extraction + clustering\n",
    "    emb = extract_embeddings_with_hook(model, data_test, device)\n",
    "    ari, nmi = run_clustering_eval(emb, Y_test, n_clusters=len(np.unique(Y_test)))\n",
    "\n",
    "    # Store all results\n",
    "    test_results[test_pid] = {\n",
    "        'loss': float(test_loss),\n",
    "        'accuracy': float(test_acc),\n",
    "        'ARI': ari,\n",
    "        'NMI': nmi\n",
    "    }\n",
    "    print(f\"Test PID {test_pid} | Acc: {test_acc:.4f} | ARI: {ari:.4f} | NMI: {nmi:.4f}\")\n",
    "\n",
    "# Save to JSON\n",
    "with open(os.path.join(results_dir, 'test_results.json'), 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}